\chapter{Conclusions}
\label{ch5:conclusion}

\section{Express Numeric Concepts}
\label{sec5.1:numeric_represent}

With all experimental results shown in Chapter \ref{ch4:results_analysis}, we could conclude that the models illustrated in Chapter \ref{ch3:game_model} can successfully transmit numeric concepts in whichever Set-Reconstruct or Set-Select game proposed in Chapter \ref{ch3:game_model}. Although the emergent languages are not compositional from the perspective of humans, they do capture the underlying structure of meaning space and reflect it into messages consist of sequences of discrete symbols, which is measured by the Euclidean distances between meaning pairs. Furthermore, the messages expressing same numeric concepts have higher similarities to each other, which is measured the BLEU score defined in Section \ref{sec3.3:metrics}. More importantly, the emergent languages can be successfully generalised to unseen meanings and they are not only effective but also efficient, as models can fit to them faster than other languages.

Therefore, we claim that the agents capture the numeric concepts during cooperating to complete the games , and successfully transmit these numeric concepts with a non-natural language.

\section{Role of Iterated Learning}
\label{sec5.2:iterated_learning}

By transforming iterated learning to train our DL-based agents, it successfully improves the compositionality of emergent languages, which is measured by Euclidean distances in meaning space and BLEU score in message space, in our original set representations of objects. Then, by taking vectors that directly encode quantities of different kinds of objects as the input for speakers, the emergent languages become almost perfectly compositional under iterated learning.

Therefore, we claim that iterated learning is an effective method to improve the compositionality of emergent languages, w/o inherently disentangled feature representation of inputs. Even thought the emergent compositionality may not correspond to what it is in human natural languages.

\section{Future Works}
\label{sec5.2:future_work}

With the current exploration, there are still several open questions in our work and thus several interesting and meaningful future works:

\begin{enumerate}
    \item \textbf{Generalisation and meta-learning}: \cite{smith2013linguistic} claims that language structure is an evolutionary trade-off between simplicity and expressivity. We assume that generalisation is another form of this trade-off. Further, emergence of numerals is a good candidate for discovering the role of generalising pressure in language evolution, as numerals can be used for whatever kind of objects. More importantly, such pressure can be formalised by meta-learning.
    \item \textbf{Feature representations}: As discussed in Section \ref{sec4.4:represent_effect}, different kinds of representations have a strong effect on the compositional form of emergent languages. Argued by \cite{locatello2018challenging}, representation learnt without supervision are not disentangled. We further assume that inherently disentangled elements are not only important in the input feature space but also in the parameter feature space. Or, to say, some words in our natural languages directly correspond to elements in input feature representations, while others may correspond the features of specific functions, e.g. agents need to learn counting (a function) in our games.
\end{enumerate}