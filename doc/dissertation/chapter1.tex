\chapter{Introduction}
\label{ch1:intro}

Natural language processing (NLP) is an important and long-standing topic in artificial intelligence (AI), in which a core question is natural language understanding (NLU). With the rapid development of deep learning (DL), most current state-of-the-art methods in NLP, e.g. \cite{socher2013recursive, word2vec2013, kim2014cnn}, are based on DL models trained on massive static textual corpora. From an information processing perspective, the information flow of NLP-based human-computer interaction systems is illustrated in Figure \ref{fig1:nlpdiagram} given as follow. As the diagram shows, the input of NLP systems are various kinds of textual materials generated by human beings to describe their experiences and perceptions. Under such a perspective, symbols in natural languages are actually feature representations of the original experiences and perceptions, whereas most current NLP systems directly take these symbols as original features.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/nlpdiagram.pdf}
  \caption{An overview of information flow in current NLP systems.}
  \label{fig1:nlpdiagram}
\end{figure}

Considering the missing original experiences and perceptions, grounded language learning (GLL) argues that models need a grounded environment to learn and understand language\cite{matuszek2018grounded}. However, natural languages of the time have been developed for at least tens of thousands of years\cite{berwick2016only} and already became very sophisticated. Thus, to verify that computational agents can truly ground symbols in natural languages to corresponding experiences and perceptions, as well as be able to complete the specified tasks, it is necessary to help them to discover and develop various kinds of characteristics of natural language during autonomous communication of agents. There are already lots of works, e.g. \cite{batali1998computational, christiansen2003language, smith2003complex, hill2017understanding, havrylov2017emergence, yu2018interactive, kottur2017natural}, aiming to facilitate the emergence of ``natural language'' in multi-agent autonomous communication systems. However, one significant limitation of previous works is that, only referential objects/attributes in environments, e.g. shapes and colours, were considered and to which discrete symbols were grounded.

This project, on the other hand, aims to explore and analyse the grounding of abstractions which are non-referential\footnote{By non-referential, we mean that ``there is no concrete entity in the world (real or virtual) can be referred as".} in the original experiences and perceptions of human beings. However, as it is too huge a topic to tackle, our project is limited to numeric concepts which correspond to cardinal numerals in natural languages for the following reasons: i) numeral systems are relatively simple and self-contained\cite{james1999numeral}; ii) concepts related to cardinal numerals are more straightforward to model with numeric representations; iii) functions of emergent cardinal numerals can be formalised and verified more reliably in simulation.

In this work, our main contributions are given as follows:

\begin{enumerate}
  \item We propose a language game in which we can verify whether computational agents can communicate numeric concepts with each other, and successfully train agents to ``invent'' communication protocols that can autonomously transmit numeric concepts.
  \item We further analyse and discuss the structure of the emergent communication protocols, and improve the compositionality by transforming iterated leaning proposed by \cite{smith2003iterated} to train the DL models.
  \item We compare learning speeds of various kinds of languages as well as different representations, and propose an alternative hypothesis for explaining the emergence of words with different types and functions.
\end{enumerate}

All our codes are released at \href{https://github.com/Shawn-Guo-CN/EmergentNumerals}{https://github.com/Shawn-Guo-CN/EmergentNumerals}.