%%%%%%%%%%%%%%%%%%%%%%%%
% Sample use of the infthesis class to prepare an MSc thesis.
% This can be used as a template to produce your own thesis.
% Date: June 2019
%
%
% The first line specifies style options for taught MSc.
% You should add a final option specifying your degree.
% *Do not* change or add any other options.
%
% So, pick one of the following:
% \documentclass[msc,deptreport,adi]{infthesis}     % Adv Design Inf
% \documentclass[msc,deptreport,ai]{infthesis}      % AI
% \documentclass[msc,deptreport,cogsci]{infthesis}  % Cognitive Sci
% \documentclass[msc,deptreport,cs]{infthesis}      % Computer Sci
% \documentclass[msc,deptreport,cyber]{infthesis}   % Cyber Sec
% \documentclass[msc,deptreport,datasci]{infthesis} % Data Sci
% \documentclass[msc,deptreport,di]{infthesis}      % Design Inf
% \documentclass[msc,deptreport,inf]{infthesis}     % Informatics
%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[msc,deptreport]{infthesis} % Do not change except to add your degree (see above).
\usepackage{breakcites}
\usepackage[pdftex]{graphicx}
\usepackage{xcolor}
\usepackage{url}

\begin{document}
\begin{preliminary}

\title{Emergence of Numerals in Multi-Agent Autonomous Communication System}

\author{Shangmin Guo}

\abstract{
  This project aims to propose a new computational simulation method for the emergence of numerals based on multi-agent autonomous communication system following deep reinforcement learning methodology. 
}

\maketitle

\section*{Acknowledgements}
Any acknowledgements go here. 

\tableofcontents
\end{preliminary}


\chapter{Introduction}
\label{ch1:intro}

Natural language processing (NLP) is an important and long-standing topic in artificial intelligence (AI), in which a core question is natural language understanding (NLU). With the rapid development of deep learning (DL), most current statae-of-the-art methods in NLP, e.g. \cite{socher2013recursive, word2vec2013, kim2014cnn}, are based on DL models trained on massive static textual corpora. From an information processing perspective, the information flow of NLP-based human-computer interaction systems is illustrated in Figure \ref{fig1:nlpdiagram} given as follow. As the diagram shows, the input of NLP systems are various kinds of textual materials generated by human beings to descibe their experiences/perceptions (E/P). Under such a perspective, symbols in natural languages are actually feature representations of the origianl E/P, whereas most current NLP systems directly take these symbols as original features.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/nlpdiagram.pdf}
  \caption{An overview of information flow in current NLP systems.}
  \label{fig1:nlpdiagram}
\end{figure}

Considering the missing original E/P, grounded language learning (GLL) argues that models need a grounded environment to learn and understand language\cite{matuszek2018grounded}. However, natural languages of the time have been developed for at least tens of thousands of years\cite{berwick2016only} and already became very sophisticated. Thus, to verify that computational agents can truly ground symbols in natural languages to correspongding E/P and can complete the specified tasks, it is necessary to facillitate them to discover and develop various kinds of characteristics of natural language during autonomous communication of agents. There are already lots of works, e.g. \cite{hill2017understanding, havrylov2017emergence, yu2018interactive, kottur2017natural}, aiming to facillitate the emergence of ``natural language'' in multi-agent autonomous communication systems. However, one significant limitation of previous works is that, only referential objects/attributes in environments, e.g. shapes and colors, were considered and to which discrete symbols were grounded to.

This project, on the other hand, aims to explore and analyse the grounding of abstractions which are non-referential in the original experiences/perceptions of human beings. However, as it is too huge a topic to tackle, our project is limited to cardinal numerals for the following reasons: i) numeral systems are relatively simple and self-contained\cite{james1999numeral}; ii) concepts related to cardinal numerals are more straightforward to model with numeric representations; iii) functions of emergent cardinal numerals can be formalised and verified more reliably in simulation.

In this work, our main contributions are given as follows:

\begin{enumerate}
  \item We are about the first to explore the problem of transmitting numerical concepts which corresponds to function words in natural languages in multi-agent autonomous communication system.
  \item We transformed iterated learning proposed by \cite{smith2003iterated} to train DL models and successfully facillitate computational agents to transmit numerical concepts.
  \item We further analyse and discuss the compositionality of the emergent communication protocal and \textcolor{red}{HERE!}.
\end{enumerate}

%------------------------------------------------------------------------------------------%

\chapter{Background}
\label{ch2:background}

As we demonstrate in Chapter \ref{ch1:intro}, there are 2 almost disjointly developed research topics that motivates this project, i.e. computer simulation methods in evolutionary linguistics and multi-agent games in GLL. Thus, in the following 2 sections, we will introduce works which are highly related to our project from these 2 different areas.

\section{Computer Simulation Methods in Evolutionary Linguistics}
\label{sec2.1:evolang}

The emergence and evolution of natural language have always been critical questions to the field of evolutionary linguistics \cite{macwhinney2013emergence} and one important issue is to use quantitative methods to overcome the time limit on unpreserved pre-historic linguistic behaviors\cite{lieberman2006toward, evans2009myth}. Since it was first introduced by \cite{hurford1989biological}, computer simulation methods have attracted a rapidly growing attention, e.g. \cite{hurford1998approaches, knight2000evolutionary, briscoe2002book, christiansen2003language, bickerton2009biological, cangelosi2012simulating}. As we introduced in Chapter \ref{ch1:intro}, one of our objectives is to facillitate computational agents to discover and develop various kinds of natural langauge phenonmana, which shares a same objective and motivation of computer simulation methods in evolutionary linguistics.

To imply and verify linguistics theories, there are 2 necessary component: i) environments, in which agents can execute actions and communicate with each other; ii) pre-defined elementary linguistic knowledge that can be manipulated and altered by agents. Further, we could categorise the environments into the following 3 different types according to their simulation objectives:
\begin{itemize}
  \item \textit{Iterated learning} introduced by \cite{kirby1999function} which aims at simulating cultural transitions from generation to generation.
  \item \textit{Language games} introduced by \cite{wittgenstein2009philosophical} which takes the emergent communication protocal in cooperation between individuals as a prototype of language.
  \item \textit{Genetic evolution} introduced by \cite{briscoe1998language} which aims at simluating evolution of languages as a kind of natural selection procedure\cite{darwin1859origin}.
\end{itemize}

With environments and pre-defined elementary linguistic knowledge, computational agents can then learn bi-directional meaningâ€“utterance mapping functions\cite{gong2013computer}. With diffrent kinds of resulting linguistic phenonmana, this simulation procedure can be broadly categorised intto 2 classes:
\begin{itemize}
  \item lexical models, e.g. \cite{steels2005emergence, baronchelli2006sharp, puglisi2008cultural}, whose main concern is whether a common lexicon can form during the communication in agent community;
  \item syntactic and grammatical models, e.g. \cite{kirby1999function, vogt2005acquisition}, in which agents mainly aim to map meanings (represented in various ways) to utterances (either structured or unstructured ).
\end{itemize}

However, no matter how these mapping functions are learnt, e.g. by neural network models \cite{munroe2002learning} or by mathematical equations \cite{minett2008modelling, ke2008language}, the most basic elements of linguistics, e.g. meanings to communicate about and a signalling channel to employ, are all pre-defined.

In contract, although we also follow the framework of language games and train agents in an iterated learning fashion, the basic linguistics elements in our project are not provided from the outset any more and computational agents can specify meanings of symbols/utterances by themselves.

\section{Multi-agent Games in Grounded Language Learning}
\label{sec2.2:gll}

Unlike how we human beings learn and understand language, the current DL-based NLP techiniques learn semantics from only large-scaled static textual materials. Thus, GLL argues that computational also need to learn and understand languages by interacting with environments and grounding language into their E/P. With the recent rapid development of deep reinforcement learning (DRL), it is proven that computational agents can master a variaty of complex cognitive activities, e.g. \cite{mnih2015human, silver2017mastering}]. Thus, a bunch of works in GLL apply DRL techiniques to facilitate agents to learn or invent natural languages\footnote{Strictly speaking, ``invent natural language'' should be called as ``invent communication protocals sharing compoitionality like natural languages''. However, as our project is to facilitate compositionality in multi-agent communication protocals, we thus call these emergent communication protocals a kind of ``langauge" invented by agents}, such as \cite{hermann2017grounded, mordatch2018emergence, havrylov2017emergence, hill2017understanding}.

To verify language abilities of computational agents, previous works in GLL usually follow the framework of language games, of which are mainly variants of referential games introduced by \cite{lewis2008convention}, e.g. \cite{hermann2017grounded, havrylov2017emergence}. Also, some works are more motivated by the competence instead of cooperation such as \cite{cao2018emergent}.

From another perspetive, based on the number of participated agents, we can broadly categorise language games in GLL into the following 2 types:
\begin{itemize}
  \item \textit{Single-agent games} usually need to be done by one agent and a human participator, in which the main concern is to explore how could computational agents learn the compositionality of semantics.
  \item \textit{Multi-agent games} are usally completed by an agent population, in which the main concern is to explore how various kinds of natural language phenonmana emerge and envolve during communicating among agents.
\end{itemize}

However, like we mentioned in Chapter \ref{ch1:intro}, whichever kind of langauge game they follow in previous works of GLL, the objects/attributes the symbols grounded to are all referential. We, on the other hand, aim to explore the feasibility of grounding symbols to non-referential objects (specifically, numerals) during the game.

%------------------------------------------------------------------------------------------%

\chapter{Set Generation Game and Models}
\label{ch3:game_model}

\section{Game Description}
\label{sec3.1:game_description}

One hypothesis of our work is that, the linguistic hypotheses can by implied by game dynamics.

\subsection{Game Procedure}
\label{ssec3.1.1:game_procedure}

\subsection{Numerals in the Game}
\label{ssec3.1.2:numeral_in_game}

\section{Proposed Models}
\label{sec3.2:models}

\subsection{Set2Seq2Seq Models}
\label{ssec3.2.1:set2seq2seq}

\subsection{Numeral Iterated Learning}
\label{ssec3.2.2:3phase}

\subsection{Baseline Models}
\label{ssec3.2.3:baselines}

%------------------------------------------------------------------------------------------%

\chapter{Experiment Results and Analysis}
\label{ch4:results_analysis}

%------------------------------------------------------------------------------------------%

\chapter{Conclusions}
\label{ch5:conclusion}

\section{Final Reminder}

The body of your dissertation, before the references and any appendices,
\emph{must} finish by page~40. The introduction, after preliminary material,
should have started on page~1.

You may not change the dissertation format (e.g., reduce the font
size, change the margins, or reduce the line spacing from the default
1.5 spacing). Over length or incorrectly-formatted dissertations will
not be accepted and you would have to modify your dissertation and
resubmit.  You cannot assume we will check your submission before the
final deadline and if it requires resubmission after the deadline to
conform to the page and style requirements you will be subject to the
usual late penalties based on your final submission time.

\section{Further Discussion}
\label{sec5.1:discusstion}

%------------------------------------------------------------------------------------------%

\bibliographystyle{apalike}
\bibliography{main}

\chapter*{Appendices}

%% You can include appendices like this:
% \appendix
% 
% \chapter{First appendix}
% 
% \section{First section}
% 
% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

\end{document}
