%%%%%%%%%%%%%%%%%%%%%%%%
% Sample use of the infthesis class to prepare an MSc thesis.
% This can be used as a template to produce your own thesis.
% Date: June 2019
%
%
% The first line specifies style options for taught MSc.
% You should add a final option specifying your degree.
% *Do not* change or add any other options.
%
% So, pick one of the following:
% \documentclass[msc,deptreport,adi]{infthesis}     % Adv Design Inf
% \documentclass[msc,deptreport,ai]{infthesis}      % AI
% \documentclass[msc,deptreport,cogsci]{infthesis}  % Cognitive Sci
% \documentclass[msc,deptreport,cs]{infthesis}      % Computer Sci
% \documentclass[msc,deptreport,cyber]{infthesis}   % Cyber Sec
% \documentclass[msc,deptreport,datasci]{infthesis} % Data Sci
% \documentclass[msc,deptreport,di]{infthesis}      % Design Inf
% \documentclass[msc,deptreport,inf]{infthesis}     % Informatics
%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[msc,deptreport]{infthesis} % Do not change except to add your degree (see above).
\usepackage{breakcites}
\usepackage[pdftex]{graphicx}
\usepackage{xcolor}
\usepackage{url}

\begin{document}
\begin{preliminary}

\title{Emergence of Numerals in Multi-Agent Autonomous Communication System}

\author{Shangmin Guo}

\abstract{
  This project aims to propose a new computational simulation method for the emergence of numerals based on multi-agent autonomous communication system following deep reinforcement learning methodology. 
}

\maketitle

\section*{Acknowledgements}
Any acknowledgements go here. 

\tableofcontents
\end{preliminary}


\chapter{Introduction}
\label{ch1:intro}

Natural language processing (NLP) is an important and long-standing topic in artificial intelligence (AI), in which a core question is natural language understanding (NLU). With the rapid development of deep learning (DL), most current statae-of-the-art methods in NLP, e.g. \cite{socher2013recursive, word2vec2013, kim2014cnn}, are based on DL models trained on massive static textual corpora. From an information processing perspective, the information flow of NLP-based human-computer interaction systems is illustrated in Figure \ref{fig1:nlpdiagram} given as follow. As the diagram shows, the input of NLP systems are various kinds of textual materials generated by human beings to descibe their experiences/perceptions (E/P). Under such a perspective, symbols in natural languages are actually feature representations of the original E/P, whereas most current NLP systems directly take these symbols as original features.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/nlpdiagram.pdf}
  \caption{An overview of information flow in current NLP systems.}
  \label{fig1:nlpdiagram}
\end{figure}

Considering the missing original E/P, grounded language learning (GLL) argues that models need a grounded environment to learn and understand language\cite{matuszek2018grounded}. However, natural languages of the time have been developed for at least tens of thousands of years\cite{berwick2016only} and already became very sophisticated. Thus, to verify that computational agents can truly ground symbols in natural languages to correspongding E/P and can complete the specified tasks, it is necessary to facilitate them to discover and develop various kinds of characteristics of natural language during autonomous communication of agents. There are already lots of works, e.g. \cite{hill2017understanding, havrylov2017emergence, yu2018interactive, kottur2017natural}, aiming to facilitate the emergence of ``natural language'' in multi-agent autonomous communication systems. However, one significant limitation of previous works is that, only referential objects/attributes in environments, e.g. shapes and colors, were considered and to which discrete symbols were grounded to.

This project, on the other hand, aims to explore and analyse the grounding of abstractions which are \textcolor{red}{non-referential (?)} in the original experiences/perceptions of human beings. However, as it is too huge a topic to tackle, our project is limited to cardinal numerals for the following reasons: i) numeral systems are relatively simple and self-contained\cite{james1999numeral}; ii) concepts related to cardinal numerals are more straightforward to model with numeric representations; iii) functions of emergent cardinal numerals can be formalised and verified more reliably in simulation.

In this work, our main contributions are given as follows:

\begin{enumerate}
  \item We propose a language game in which we can define numerals as symbols indicating numbers of replicating tokens when generating outputs.
  \item Based on the language game, we successfully train agents to transmit numerical concepts which corresponds to function words in natural languages.
  \item We transformed iterated learning proposed by \cite{smith2003iterated} to train DL models and find that it does help to improve the compoitionality of the emergent language.
  \item We further analyse and discuss the compositionality of the emergent communication protocal and \textcolor{red}{HERE!}.
\end{enumerate}

%------------------------------------------------------------------------------------------%

\chapter{Background}
\label{ch2:background}

As we demonstrate in Chapter \ref{ch1:intro}, there are 2 almost disjointly developed research topics that motivates this project, i.e. computer simulation methods in evolutionary linguistics and multi-agent games in GLL. Thus, in the following 2 sections, we will introduce works which are highly related to our project from these 2 different areas.

\section{Computer Simulation Methods in Evolutionary Linguistics}
\label{sec2.1:evolang}

The emergence and evolution of natural language have always been critical questions to the field of evolutionary linguistics \cite{macwhinney2013emergence} and one important issue is to use quantitative methods to overcome the time limit on unpreserved pre-historic linguistic behaviors\cite{lieberman2006toward, evans2009myth}. Since it was first introduced by \cite{hurford1989biological}, computer simulation methods have attracted a rapidly growing attention, e.g. \cite{hurford1998approaches, knight2000evolutionary, briscoe2002book, christiansen2003language, bickerton2009biological, cangelosi2012simulating}. As we introduced in Chapter \ref{ch1:intro}, one of our objectives is to facilitate computational agents to discover and develop various kinds of natural langauge phenonmana, which shares a same objective and motivation of computer simulation methods in evolutionary linguistics.

To imply and verify linguistics theories, there are 2 necessary component: i) environments, in which agents can execute actions and communicate with each other; ii) pre-defined elementary linguistic knowledge that can be manipulated and altered by agents. Further, we could categorise the environments into the following 3 different types according to their simulation objectives:
\begin{itemize}
  \item \textit{Iterated learning} introduced by \cite{kirby1999function} which aims at simulating cultural transitions from generation to generation.
  \item \textit{Language games} introduced by \cite{wittgenstein2009philosophical} which takes the emergent communication protocal in cooperation between individuals as a prototype of language.
  \item \textit{Genetic evolution} introduced by \cite{briscoe1998language} which aims at simluating evolution of languages as a kind of natural selection procedure\cite{darwin1859origin}.
\end{itemize}

With environments and pre-defined elementary linguistic knowledge, computational agents can then learn bi-directional meaningâ€“utterance mapping functions\cite{gong2013computer}. With diffrent kinds of resulting linguistic phenonmana, this simulation procedure can be broadly categorised intto 2 classes:
\begin{itemize}
  \item lexical models, e.g. \cite{steels2005emergence, baronchelli2006sharp, puglisi2008cultural}, whose main concern is whether a common lexicon can form during the communication in agent community;
  \item syntactic and grammatical models, e.g. \cite{kirby1999function, vogt2005acquisition}, in which agents mainly aim to map meanings (represented in various ways) to utterances (either structured or unstructured ).
\end{itemize}

However, no matter how these mapping functions are learnt, e.g. by neural network models \cite{munroe2002learning} or by mathematical equations \cite{minett2008modelling, ke2008language}, the most basic elements of linguistics, e.g. meanings to communicate about and a signalling channel to employ, are all pre-defined.

In contract, although we also follow the framework of language games and train agents in an iterated learning fashion, the basic linguistics elements in our project are not provided from the outset any more and computational agents can specify meanings of symbols/utterances by themselves.

\section{Multi-agent Games in Grounded Language Learning}
\label{sec2.2:gll}

Unlike how we human beings learn and understand language, the current DL-based NLP techiniques learn semantics from only large-scaled static textual materials. Thus, GLL argues that computational also need to learn and understand languages by interacting with environments and grounding language into their E/P. With the recent rapid development of deep reinforcement learning (DRL), it is proven that computational agents can master a variaty of complex cognitive activities, e.g. \cite{mnih2015human, silver2017mastering}]. Thus, a bunch of works in GLL apply DRL techiniques to facilitate agents to learn or invent natural languages\footnote{Strictly speaking, ``invent natural language'' should be called as ``invent communication protocals sharing compoitionality like natural languages''. However, as our project is to facilitate compositionality in multi-agent communication protocals, we thus call these emergent communication protocals a kind of ``langauge" invented by agents}, such as \cite{hermann2017grounded, mordatch2018emergence, havrylov2017emergence, hill2017understanding}.

To verify language abilities of computational agents, previous works in GLL usually follow the framework of language games, of which are mainly variants of referential games introduced by \cite{lewis2008convention}, e.g. \cite{hermann2017grounded, havrylov2017emergence}. Also, some works are more motivated by the competence instead of cooperation such as \cite{cao2018emergent}.

From another perspetive, based on the number of participated agents, we can broadly categorise language games in GLL into the following 2 types:
\begin{itemize}
  \item \textit{Single-agent games} usually need to be done by one agent and a human participator, in which the main concern is to explore how could computational agents learn the compositionality of semantics.
  \item \textit{Multi-agent games} are usally completed by an agent population, in which the main concern is to explore how various kinds of natural language phenonmana emerge and envolve during communicating among agents.
\end{itemize}

However, like we mentioned in Chapter \ref{ch1:intro}, whichever kind of langauge game they follow in previous works of GLL, the objects/attributes the symbols grounded to are all referential. We, on the other hand, aim to explore the feasibility of grounding symbols to non-referential objects (specifically, numerals) during the game.

%------------------------------------------------------------------------------------------%

\chapter{Game, Models and Evaluation Methods}
\label{ch3:game_model}

In this chapter, we first describe the proposed language game and the definition of numerals in our game. We then introduce the architecture of models we used and also the iterated learning for training models.

\section{Game Description}
\label{sec3.1:game_description}

Unlike traditional simulation methods in evolutionary linguistics introduced in Section \ref{sec2.1:evolang}, there are 3 necessary components in our architecture and they are given as follows:

\begin{itemize}
  \item \textit{Environment}: To imply our linguistic assumption as well as make the size of environment limited and thus analysable, all perceptions in the established environment are sequences of objects represented by one-hot vectors. For ease of demonstration, we denote these objects as $o \in \{ A, B, C, D\}$ in the following sections.
  \item \textit{Agents}: There are 2 kinds of agents in our project: i) \textit{speakers} $S$ that can observe objects in the environment and emit messages $m_i$; ii) \textit{listeners} $L$ that can receive the messages and generate a sequence of obejcts.
  \item \textit{Dynamics}: In this project, the dynamics mean not only the manually designed reward function for agents but also the training mechanism, e.g. iterated learning and blank vocabulary. The details will be introduced in Subsection \ref{ssec3.2.4:loss_reward} and Subsection \ref{ssec3.2.2:3phase}. It worth mentioning that one premise of our project is that all the linguistic hypotheses are purely implied by dynamics.
\end{itemize}

\subsection{Game Procedure}
\label{ssec3.1.1:game_procedure}

The overall view of the proposed game is illustrated in Figure \ref{fig2:game_procedure} given as follow.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/task_illustration.pdf}
  \caption{Diagram of Game Playing Procedure.}
  \label{fig2:game_procedure}
\end{figure}

According to the steps of playing games at iteration $i$, the components of our games are illustrated as follows:
\begin{enumerate}
  \item Perceptions: the perception from environments is a \textbf{set} of objecst, i.e. $s_i=\{o_{i_1}, o_{i_2}, \dots\}$.
  \item Speaker observation and message generation: after observing and encoding the perception, speaker $S$ would generate a message $m_i=\{s_{i_1}, s_{i_2}, \dots, s_{M}\}$ where $M$ is the maximum length of messages and $s_k$ are selected from a randomly initialised vocabulary such that the symbols in the initial vocabulary are meaningless;
  \item Listener receiving message and perception reproduction: after receiving and encoding the message $m_i$, the listener would generate a \textbf{sequence} $\hat{s}_i = \{\hat{o}_{i_1}, \hat{o}_{i_2}, \dots\}$ whose symbols are identical to those in the original perception $s_i$;
  \item Reward and parameter update: by comparing $s_i$ and $\hat{s}_i$, we take the cross-entropy between them as the reward for both listener and speaker and update parameters of both speaker and listener with respect to it.\footnote{Different ways of updating parameters are instroduced in Section \ref{sec3.2:models}.}
\end{enumerate}

One thing that needs to be highlighted is that the perceptions $s_i$ are sets and thus order of objects would not make any diffrence. Further, we argue that the only important feature that need to be transmitted is actually the numbers of different objects which correponds to the function of numerals in natural language.

\subsection{Numerals in the Game}
\label{ssec3.1.2:numeral_in_game}

\section{Proposed Models}
\label{sec3.2:models}

\subsection{Set2Seq2Seq Models}
\label{ssec3.2.1:set2seq2seq}

\subsection{Numeral Iterated Learning}
\label{ssec3.2.2:3phase}

\subsection{Baseline Models}
\label{ssec3.2.3:baselines}

\subsection{Loss and Reward}
\label{ssec3.2.4:loss_reward}

\section{Evaluation Methods}
\label{sec3.3:evaluation}



%------------------------------------------------------------------------------------------%

\chapter{Experiment Results and Discussion}
\label{ch4:results_analysis}

\section{Future Works}
\label{sec4.2:future_work}

%------------------------------------------------------------------------------------------%

\chapter{Conclusions}
\label{ch5:conclusion}

\section{Final Reminder}

The body of your dissertation, before the references and any appendices,
\emph{must} finish by page~40. The introduction, after preliminary material,
should have started on page~1.

You may not change the dissertation format (e.g., reduce the font
size, change the margins, or reduce the line spacing from the default
1.5 spacing). Over length or incorrectly-formatted dissertations will
not be accepted and you would have to modify your dissertation and
resubmit.  You cannot assume we will check your submission before the
final deadline and if it requires resubmission after the deadline to
conform to the page and style requirements you will be subject to the
usual late penalties based on your final submission time.

\section{Further Discussion}
\label{sec5.1:discusstion}

%------------------------------------------------------------------------------------------%

\bibliographystyle{apalike}
\bibliography{main}

\chapter*{Appendices}

%% You can include appendices like this:
% \appendix
% 
% \chapter{First appendix}
% 
% \section{First section}
% 
% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

\end{document}
