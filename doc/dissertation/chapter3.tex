\chapter{Game, Models and Evaluation Methods}
\label{ch3:game_model}

In this chapter, we first describe the proposed language game and the definition of numerals in our game. We then introduce the architecture of models we used and also the iterated learning for training models.

\section{Game Description}
\label{sec3.1:game_description}

Unlike traditional simulation methods in evolutionary linguistics introduced in Section \ref{sec2.1:evolang}, there are 3 necessary components in our architecture and they are given as follows:

\begin{itemize}
  \item \textit{Environment}: To imply our linguistic assumption as well as make the size of environment limited and thus analysable, all perceptions in the established environment are sequences of objects represented by one-hot vectors. For ease of demonstration, we denote these objects as $o \in \{ A, B, C, D\}$ in the following sections.
  \item \textit{Agents}: There are 2 kinds of agents in our project: i) \textit{speakers} $S$ that can observe objects in the environment and emit messages $m_i$; ii) \textit{listeners} $L$ that can receive the messages and generate a sequence of obejcts.
  \item \textit{Dynamics}: In this project, the dynamics mean not only the manually designed reward function for agents but also the training mechanism, e.g. iterated learning and blank vocabulary. The details will be introduced in Subsection \ref{ssec3.2.4:loss_learning} and Subsection \ref{ssec3.2.5:iterated_learning}. It worth mentioning that one premise of our project is that all the linguistic hypotheses are purely implied by dynamics.
\end{itemize}

\subsection{Game Procedure}
\label{ssec3.1.1:game_procedure}

The overall view of the proposed Set-Forward game is illustrated in Figure \ref{fig2:game_procedure} given as follow.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/task_illustration.pdf}
  \caption{Diagram of Game Playing Procedure.}
  \label{fig2:game_procedure}
\end{figure}

According to the steps of playing games at iteration $i$, the components of our games are illustrated as follows:
\begin{enumerate}
  \item Perceptions: the perception from environments is a \textbf{set} of objects, i.e. $s_i=\{o_{i_1}, o_{i_2}, \dots\}$.
  \item Speaker observation and message generation: after observing and encoding the perception, speaker $S$ would generate a message $m_i=\{s_{i_1}, s_{i_2}, \dots, s_{M}\}$ where $M$ is the maximum length of messages and $s_k$ are selected from a randomly initialised vocabulary such that the symbols in the initial vocabulary are meaningless;
  \item Listener receiving message and perception reproduction: after receiving and encoding the message $m_i$, the listener would generate a \textbf{sequence} $\hat{s}_i = \{\hat{o}_{i_1}, \hat{o}_{i_2}, \dots\}$ whose symbols are identical to those in the original perception $s_i$;
  \item Reward and parameter update: by comparing $s_i$ and $\hat{s}_i$, we take the cross-entropy between them as the reward for both listener and speaker and update parameters of both speaker and listener with respect to it.\footnote{Different ways of updating parameters are instroduced in Section \ref{sec3.2:models}.}
\end{enumerate}

One thing that needs to be highlighted is that the perceptions $s_i$ are sets and thus order of objects would not make any diffrence. Further, we argue that the only important feature that need to be transmitted is actually the numbers of different objects which correponds to the function of numerals in natural language.

\subsection{Functions of Numerals in the Game}
\label{ssec3.1.2:numeral_in_game}

Broadly speaking, numerals are words that can describe the numerical quantities and usually act as determiners to specify the quantities of nouns, e.g. "two dogs" and "three people". Also, under most scenarios, numerals correpond to non-referential concepts\cite{da2016wow}. Considering the objective of listeners $L$ in our language game, we define a numeral as a symbol $s^n$ at \textbf{position} $i$ indicating a function that reproduce some object $o_i$ exactly $n$ times:

\begin{equation}
  s^n: o_i \rightarrow \{\overbrace{o_i, \dots, o_i}^{n \mbox{ elements}}\}
  \label{eq:3.1numeral_define}
\end{equation}

Note that, the meaning of a symbol is not only decided by itself but also its position in message, as $L$ would encode meanings of symbols according to their appearance in messages. Also, in our models, there is no specific mechanism to separate the meanings of symbols from their positions.

From the side of speakers $S$, a numeral preferred to be defined as a symbol $s^n$ at \textbf{position} $i$ that represents the numbers of specific object $o_i$. Thus, we expect $S$ would first learn to count the number of different objects and then encode them into a sequence of discrete symbols. As \cite{Siegelmann1992NN} shows that Recurrent Neural Networks (RNNs) are Turing-complete and Long-short Term Memory (LSTM) model proposed by \cite{hochreiter1997long} is a super set of RNN, it is safe to claim that LSTM is also Turing-complete and thus capable of counting numbers of objects.

\subsection{A Variant: Set-Select Game}
\label{ssec:3.1.3:refer_game}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/setrefer_game.pdf}
  \caption{Diagram of Referential Game Playing Procedure.}
  \label{fig3:refer_game_procedure}
\end{figure}

We illustrate the Set-Select game, a variant of Set-Forward game, in Figure \ref{fig3:refer_game_procedure} given above. The only difference is that listeners need to select the correct set of objects among a bunch of distractors\footnote{A distractor is a set that contains different numbers of objects as the correct one.} instead of generating it. 

\section{Proposed Models}
\label{sec3.2:models}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\textwidth]{graphs/set2seq2_.pdf}
  \caption{Overall Diagram of Model Architectures for Playing Games.}
  \label{fig4:model_arch}
\end{figure}

We illustrate the overall architecture of our models in Figure \ref{fig4:model_arch} given above, in which it is straightforward to see that a speaker $S$ consists of a set encoder and a standard LSTM sequence decoder that can generate messages. As for a listeners $L$, it would first encode messages with a LSTM sequence encoder and get the feature vector $h^l_m$. Then, in the Set-Forward game, $L$ would take $h^l_m$ as the initial hidden state and predict a sequence of objects with a LSTM sequence decoder. As for in Set-Select game, $L$ would compare $h^l_m$ with a bunch of sets which are encoded by set encoders of $L$ and select the one shown to $S$ based on the dot product between $h^l_m$ and feature vectors of each set.

Further details are shown in the following subsections.

\subsection{Speaker}
\label{ssec3.2.1:speaker}

The architecture of our speaking agents is very similar to the Seq-to-Seq model proposed by \cite{sutskever2014sequence} except that replace the encoder for input sequences with a set encoder whose details would be introduced in the following subsubsection. As Seq-to-Seq model is quite popular nowadays, we skip details about how to generate sequences which correpond to the messages in our games, and focus on how to encode sets of objects.

\subsubsection{Set Encoder}
\label{sssec3.2.1.1:set_encoder}

Our set encoder shares an almost same architecture of inputting sets proposed by \cite{vinyals2015order}. However, as there is an addition in softmax function and it would introduce counting bias into the feature representation of sets, we replace equation (5) in \cite{vinyals2015order} with the following operation in order to avoid exposing counting system to models:

\begin{equation}
  a_{i,t} = \sigma(e_{i,t})
  \label{eq3.2.1.1:sigmoid_to_replace_softmax}
\end{equation}
where $\sigma$ is sigmoid function.

In our implementation, the number of attention operations is set to be the same as the number of all types of objects, as we want to help models to represent number of each kind of objects as features in the vector representation of input set.

\subsubsection{Message Generator}
\label{sssec3.2.1.2:msg_generator}

To generate the message $m_i$, we follow \cite{havrylov2017emergence} and adopt a LSTM-based sequence decoder with 2 different kinds of sampling mechanisms: i)direct sampling that directly sample from the correponding categorical distribution specified by $softmax(Wh_k + b)$; ii) GUMBEL-softmax estimator proposed by \cite{jang2016categorical} with straight-through trick introduced in \cite{bengio2013estimating}. Beside, the learning mechanisms also vary for these 2 different sampling methods, which is further discussed in Subsection \ref{ssec3.2.4:loss_learning}.

Note that the length of each message $m_i$ is fixed to $|M|$ and symbols $s_{i_1},\dots,s_{i_|M|}$ are all from an initially meaningless vocabulary whose size is $|L|$. The effect of $|L|$ and $|M|$ on the emergent language is further discussed in Chapter \ref{ch4:results_analysis}.

\subsection{Listener in Set2Seq2Seq Model}
\label{ssec3.2.2:set2seq2seq}

\subsection{Listener in Set2Seq2Choice Model}
\label{ssec3.2.3:set2seq2choice}

\subsection{Loss/Reward and Learning}
\label{ssec3.2.4:loss_learning}

\subsection{Numeral Iterated Learning}
\label{ssec3.2.5:iterated_learning}

\subsection{Baseline Models}
\label{ssec3.2.6:baselines}

\section{Evaluation Methods}
\label{sec3.3:evaluation}
